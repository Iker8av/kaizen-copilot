{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from git import Repo\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_repo_set(jsonl_path):\n",
    "    repos = set()\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                sample = json.loads(line)\n",
    "                repo = sample.get(\"repo\")\n",
    "                if repo:\n",
    "                    repos.add(repo.strip())\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lite_repos = get_unique_repo_set(\"./swe_bench/swe_bench_lite_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lite_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_repos: list[str] = list(lite_repos)\n",
    "list_repos = sorted(list_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astropy/astropy',\n",
       " 'django/django',\n",
       " 'matplotlib/matplotlib',\n",
       " 'mwaskom/seaborn',\n",
       " 'pallets/flask',\n",
       " 'psf/requests',\n",
       " 'pydata/xarray',\n",
       " 'pylint-dev/pylint',\n",
       " 'pytest-dev/pytest',\n",
       " 'scikit-learn/scikit-learn',\n",
       " 'sphinx-doc/sphinx',\n",
       " 'sympy/sympy']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHIB_BASE_URL = \"https://github.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"BAAI/bge-large-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_files(root_dir, extensions={\".py\"}):\n",
    "    collected = []\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        if any(excluded in dirpath for excluded in [\".git\", \"node_modules\", \".venv\", \"__pycache__\"]):\n",
    "            continue\n",
    "        for fname in filenames:\n",
    "            if any(fname.endswith(ext) for ext in extensions):\n",
    "                collected.append(os.path.join(dirpath, fname))\n",
    "    return collected\n",
    "\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error leyendo {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def embed_repo(repo_path):\n",
    "    files = collect_files(repo_path)\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "    docs = []\n",
    "\n",
    "    for path in files:\n",
    "        content = read_file(path)\n",
    "        if not content.strip():\n",
    "            continue\n",
    "        \n",
    "        path_split = path.split(\"/\")\n",
    "        name = path_split[-1]\n",
    "\n",
    "        content = \"[CLS] \" + content\n",
    "        embedding = model.encode(content)  \n",
    "        embeddings.append(embedding)\n",
    "        metadata.append({\"name\": name, \"extension\": \"py\", \"path\": \"/\".join(path_split[-2:])})\n",
    "        docs.append(content)\n",
    "\n",
    "    return embeddings, metadata, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_repo(repo):\n",
    "    # 1. Crear carpeta temporal\n",
    "    repo_name = repo.split(\"/\")[-1]\n",
    "    repo_url = GITHIB_BASE_URL + repo\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        print(f\"Clonando {repo_url} en {tmp_dir}\")\n",
    "        try:\n",
    "            Repo.clone_from(repo_url, tmp_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al clonar {repo_url}: {e}\")\n",
    "            return\n",
    "        # _ = input(\"lll\")\n",
    "        embeddings, metadata, docs = embed_repo(tmp_dir )\n",
    "\n",
    "        # 4. Guardar en vector DB\n",
    "        # vector_db.add(embeddings, metadata=metadata)\n",
    "\n",
    "        # 5. tmp_dir se elimina autom√°ticamente\n",
    "        print(f\"Repo {repo_url} procesado y eliminado.\")\n",
    "        return embeddings, metadata, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.HttpClient(host=\"localhost\", port=8005, settings=Settings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/11) Repo: django/django ------------------------ \n",
      "\n",
      "Clonando https://github.com/django/django en /var/folders/c1/rzk6gvfs68l9xj63x3bl24nr0000gn/T/tmp4ndzme2t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo https://github.com/django/django procesado y eliminado.\n",
      "(1/11) Repo: matplotlib/matplotlib ------------------------ \n",
      "\n",
      "Clonando https://github.com/matplotlib/matplotlib en /var/folders/c1/rzk6gvfs68l9xj63x3bl24nr0000gn/T/tmpnioxiccf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo https://github.com/matplotlib/matplotlib procesado y eliminado.\n",
      "(2/11) Repo: mwaskom/seaborn ------------------------ \n",
      "\n",
      "Clonando https://github.com/mwaskom/seaborn en /var/folders/c1/rzk6gvfs68l9xj63x3bl24nr0000gn/T/tmpvqzdnlbg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo https://github.com/mwaskom/seaborn procesado y eliminado.\n",
      "(3/11) Repo: pallets/flask ------------------------ \n",
      "\n",
      "Clonando https://github.com/pallets/flask en /var/folders/c1/rzk6gvfs68l9xj63x3bl24nr0000gn/T/tmpqjq4hyoh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo https://github.com/pallets/flask procesado y eliminado.\n",
      "(4/11) Repo: psf/requests ------------------------ \n",
      "\n",
      "Clonando https://github.com/psf/requests en /var/folders/c1/rzk6gvfs68l9xj63x3bl24nr0000gn/T/tmpv7eysq24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo https://github.com/psf/requests procesado y eliminado.\n",
      "(5/11) Repo: pydata/xarray ------------------------ \n",
      "\n",
      "Clonando https://github.com/pydata/xarray en /var/folders/c1/rzk6gvfs68l9xj63x3bl24nr0000gn/T/tmp8lhxjtru\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo https://github.com/pydata/xarray procesado y eliminado.\n",
      "(6/11) Repo: pylint-dev/pylint ------------------------ \n",
      "\n",
      "Clonando https://github.com/pylint-dev/pylint en /var/folders/c1/rzk6gvfs68l9xj63x3bl24nr0000gn/T/tmp_bdw16n_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo https://github.com/pylint-dev/pylint procesado y eliminado.\n",
      "(7/11) Repo: pytest-dev/pytest ------------------------ \n",
      "\n",
      "Clonando https://github.com/pytest-dev/pytest en /var/folders/c1/rzk6gvfs68l9xj63x3bl24nr0000gn/T/tmp_cnwlfrp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo https://github.com/pytest-dev/pytest procesado y eliminado.\n",
      "(8/11) Repo: scikit-learn/scikit-learn ------------------------ \n",
      "\n",
      "Clonando https://github.com/scikit-learn/scikit-learn en /var/folders/c1/rzk6gvfs68l9xj63x3bl24nr0000gn/T/tmp72xj3l87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo https://github.com/scikit-learn/scikit-learn procesado y eliminado.\n",
      "(9/11) Repo: sphinx-doc/sphinx ------------------------ \n",
      "\n",
      "Clonando https://github.com/sphinx-doc/sphinx en /var/folders/c1/rzk6gvfs68l9xj63x3bl24nr0000gn/T/tmp4qf6lxjw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo https://github.com/sphinx-doc/sphinx procesado y eliminado.\n",
      "(10/11) Repo: sympy/sympy ------------------------ \n",
      "\n",
      "Clonando https://github.com/sympy/sympy en /var/folders/c1/rzk6gvfs68l9xj63x3bl24nr0000gn/T/tmp28gx_vxc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo https://github.com/sympy/sympy procesado y eliminado.\n"
     ]
    }
   ],
   "source": [
    "current_repos = list_repos[1:]\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "for idx, repo in enumerate(current_repos):\n",
    "    print(f\"({idx}/{len(current_repos)}) Repo: {repo} ------------------------ \\n\")\n",
    "    embeddings, metadata, docs = process_repo(repo)\n",
    "    collection_name = repo.replace(\"/\", \"_\")\n",
    "    \n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    for i in range(0, len(docs), BATCH_SIZE):\n",
    "        batch_docs = docs[i:i+BATCH_SIZE]\n",
    "        batch_embeddings = embeddings[i:i+BATCH_SIZE]\n",
    "        batch_metadata = metadata[i:i+BATCH_SIZE]\n",
    "        batch_ids = [str(uuid.uuid4()) for _ in batch_docs]\n",
    "\n",
    "        collection.upsert(\n",
    "            ids=batch_ids,\n",
    "            documents=batch_docs,\n",
    "            embeddings=batch_embeddings,\n",
    "            metadatas=batch_metadata\n",
    "        )    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaizen-copilot-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
